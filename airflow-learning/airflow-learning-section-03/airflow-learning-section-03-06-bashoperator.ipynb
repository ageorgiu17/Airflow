{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BashOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cadrul lecției precedente am învățat cum anume să descărcăm anumite date utilizând Python. Datele respective au fost salvate ca și fișier json pe mașina locală. În situații reale, de cele mai multe ori aceste fișier sunt prea mari ca să fie ținute pe o mașină locală. Din acest motiv următorul pas este să ne punem acest fișier în hdfs care este un fișier de date distribuit de la Apache. Ca să facem asta o să ne folosim de operatoul BashOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca să facem asta trebuie să navigăm întâi pe HUE, sistem prin care putem să facem query la fișier distribuite de tipul HDFS (o să ne salvăm fișierul json creat prin operatorul de Python ca și HDFS). Ca să deschidem HUE o să navigăm la \"localhost:32672\". Avem HUE instalat pe mașina locală cu ajutorul acelui fișier de docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../ss/airflow-section-03/section-03-ss-07.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicația de HUE se vede în imaginea de mai sus. Ce o să facem prin acest operator de bash? O să ne creem un nou director denumit \"forex\" (directorul o să fie creat în HDFS) și o să ne copiem fișierul \"forex_rates.json\" în acest director nou creat. Comenzile de bash pentru a face asta sunt următoarele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p /forex\n",
    "hdfs dfs -put -f $AIRFLOW_HOME/dags/files/forex_rates.json /forex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aceste comenzi trebuie să le introducem în cadrul operatorului de Bash. Prin operatorul respectiv, oferindu-i aceste comenzi o să ne copiem fișierul respectiv în HDFS. Codul pentru a face asta este următorul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "save_forex_rates_to_hdfs = BashOperator(\n",
    "    task_id=\"save_forex_rates_to_hdfs\",\n",
    "    bash_command=\"\"\"\n",
    "        hdfs dfs -mkdir -p /forex && \\\n",
    "        hdfs dfs -put -f $AIRFLOW_HOME/dags/files/forex_rates.json /forex\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codul cu tot DAG-ul este următrorul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define the default_args\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"retries\": 3,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"email_on_failure\": False,\n",
    "    \"email\": \"georgiuandrei05@gmail.com\"\n",
    "}\n",
    "\n",
    "# Define the function that donwloads the data from the API\n",
    "def download_rates():\n",
    "    BASE_URL = \"https://gist.githubusercontent.com/marclamberti/f45f872dea4dfd3eaa015a4a1af4b39b/raw/\"\n",
    "    ENDPOINTS = {\n",
    "        'USD': 'api_forex_exchange_usd.json',\n",
    "        'EUR': 'api_forex_exchange_eur.json'\n",
    "    }\n",
    "    with open('/opt/airflow/dags/files/forex_currencies.csv') as forex_currencies:\n",
    "        reader = csv.DictReader(forex_currencies, delimiter=';')\n",
    "        for idx, row in enumerate(reader):\n",
    "            base = row['base']\n",
    "            with_pairs = row['with_pairs'].split(' ')\n",
    "            indata = requests.get(f\"{BASE_URL}{ENDPOINTS[base]}\").json()\n",
    "            outdata = {'base': base, 'rates': {}, 'last_update': indata['date']}\n",
    "            for pair in with_pairs:\n",
    "                outdata['rates'][pair] = indata['rates'][pair]\n",
    "            with open('/opt/airflow/dags/files/forex_rates.json', 'a') as outfile:\n",
    "                json.dump(outdata, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    dag_id = 'test_forex_data_pipeline',\n",
    "    description='Testing the Data Forex Pipeline from Udemy Course',\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False\n",
    "    ) as dag:\n",
    "    \n",
    "    # Define the HttpSensor task\n",
    "    is_forex_rates_available = HttpSensor(\n",
    "        task_id=\"is_forex_rates_available\",\n",
    "        http_conn_id=\"forex_api\",\n",
    "        endpoint=\"marclamberti/f45f872dea4dfd3eaa015a4a1af4b39b\",\n",
    "        response_check=lambda response: \"rates\" in response.text,\n",
    "        poke_interval=5,\n",
    "        timeout=25\n",
    "    )\n",
    "\n",
    "    # Define the FileSensor task\n",
    "    is_forex_file_available = FileSensor(\n",
    "        task_id=\"is_forex_file_available\",\n",
    "        fs_conn_id=\"forex_path\",\n",
    "        filepath=\"forex_currencies.csv\",\n",
    "        poke_interval=5,\n",
    "        timeout=20\n",
    "    )\n",
    "\n",
    "    # Define the PythonOperator task\n",
    "    download_forex_rates = PythonOperator(\n",
    "        task_id=\"download_forex_rates\",\n",
    "        python_callable=download_rates\n",
    "    )\n",
    "\n",
    "    # Define the BashOperator\n",
    "    save_forex_rates_to_hdfs = BashOperator(\n",
    "        task_id=\"save_forex_rates_to_hdfs\",\n",
    "        bash_command=\"\"\"\n",
    "            hdfs dfs -mkdir -p /forex && \\\n",
    "            hdfs dfs -put -f $AIRFLOW_HOME/dags/files/forex_rates.json /forex\n",
    "        \"\"\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
