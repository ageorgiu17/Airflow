{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiveOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cu task-ul precedent ne-am mutat fișierul în HDFS. Datele însă de cele mai multe ori o să le stocăm în cadrul unui table, într-o bază de date. Cu ajutorul operatorului Hive o să ne creem un tabel în HDFS pentru a stoca datele. Ca să putem face acest lucru avem nevoie de următoarele:\n",
    "\n",
    "- hql = codul HQl care să fie executat\n",
    "\n",
    "- hive_cli_conn_id = conexiunea către hive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexiunea către Hive arată așa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../ss/airflow-section-03/section-03-ss-09.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De asemena, Hive a fost instalat în momentul în care am rulat acel docker-compose file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum că avem și conexiunea, putem să ne creem task-ul prin care să se creeze acest fișiser în HDFS. Codul o să arate așa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ariflow.providers.apache.hive.operators.hive import HiveOperator\n",
    "\n",
    "create_forex_rates_table_hdfs = HiveOperator(\n",
    "    task_id=\"create_forex_rates_table_hdfs\",\n",
    "    hive_cli_conn_id=\"hive_conn\",\n",
    "    hql=\"\"\"\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS forex_rates(\n",
    "            base STRING,\n",
    "            last_update DATE,\n",
    "            eur DOUBLE,\n",
    "            usd DOUBLE,\n",
    "            nzd DOUBLE,\n",
    "            gbp DOUBLE,\n",
    "            jpy DOUBLE,\n",
    "            cad DOUBLE\n",
    "            )\n",
    "        ROW FORMAT DELIMITED\n",
    "        FIELDS TERMINATED BY ','\n",
    "        STORED AS TEXTFILE\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codul final este următorul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.providers.apache.hive.operators.hive import HiveOperator\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define the default_args\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"retries\": 3,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"email_on_failure\": False,\n",
    "    \"email\": \"georgiuandrei05@gmail.com\"\n",
    "}\n",
    "\n",
    "# Define the function that donwloads the data from the API\n",
    "def download_rates():\n",
    "    BASE_URL = \"https://gist.githubusercontent.com/marclamberti/f45f872dea4dfd3eaa015a4a1af4b39b/raw/\"\n",
    "    ENDPOINTS = {\n",
    "        'USD': 'api_forex_exchange_usd.json',\n",
    "        'EUR': 'api_forex_exchange_eur.json'\n",
    "    }\n",
    "    with open('/opt/airflow/dags/files/forex_currencies.csv') as forex_currencies:\n",
    "        reader = csv.DictReader(forex_currencies, delimiter=';')\n",
    "        for idx, row in enumerate(reader):\n",
    "            base = row['base']\n",
    "            with_pairs = row['with_pairs'].split(' ')\n",
    "            indata = requests.get(f\"{BASE_URL}{ENDPOINTS[base]}\").json()\n",
    "            outdata = {'base': base, 'rates': {}, 'last_update': indata['date']}\n",
    "            for pair in with_pairs:\n",
    "                outdata['rates'][pair] = indata['rates'][pair]\n",
    "            with open('/opt/airflow/dags/files/forex_rates.json', 'a') as outfile:\n",
    "                json.dump(outdata, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    dag_id = 'test_forex_data_pipeline',\n",
    "    description='Testing the Data Forex Pipeline from Udemy Course',\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False\n",
    "    ) as dag:\n",
    "    \n",
    "    # Define the HttpSensor task\n",
    "    is_forex_rates_available = HttpSensor(\n",
    "        task_id=\"is_forex_rates_available\",\n",
    "        http_conn_id=\"forex_api\",\n",
    "        endpoint=\"marclamberti/f45f872dea4dfd3eaa015a4a1af4b39b\",\n",
    "        response_check=lambda response: \"rates\" in response.text,\n",
    "        poke_interval=5,\n",
    "        timeout=25\n",
    "    )\n",
    "\n",
    "    # Define the FileSensor task\n",
    "    is_forex_file_available = FileSensor(\n",
    "        task_id=\"is_forex_file_available\",\n",
    "        fs_conn_id=\"forex_path\",\n",
    "        filepath=\"forex_currencies.csv\",\n",
    "        poke_interval=5,\n",
    "        timeout=20\n",
    "    )\n",
    "\n",
    "    # Define the PythonOperator task\n",
    "    download_forex_rates = PythonOperator(\n",
    "        task_id=\"download_forex_rates\",\n",
    "        python_callable=download_rates\n",
    "    )\n",
    "\n",
    "    # Define the BashOperator task\n",
    "    save_forex_rates_to_hdfs = BashOperator(\n",
    "        task_id=\"save_forex_rates_to_hdfs\",\n",
    "        bash_command=\"\"\"\n",
    "            hdfs dfs -mkdir -p /forex && \\\n",
    "            hdfs dfs -put -f $AIRFLOW_HOME/dags/files/forex_rates.json /forex\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Define the HiveOperator task\n",
    "    create_forex_rates_table_hdfs = HiveOperator(\n",
    "        task_id=\"create_forex_rates_table_hdfs\",\n",
    "        hive_cli_conn_id=\"hive_conn\",\n",
    "        hql=\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS forex_rates(\n",
    "                base STRING,\n",
    "                last_update DATE,\n",
    "                eur DOUBLE,\n",
    "                usd DOUBLE,\n",
    "                nzd DOUBLE,\n",
    "                gbp DOUBLE,\n",
    "                jpy DOUBLE,\n",
    "                cad DOUBLE\n",
    "                )\n",
    "            ROW FORMAT DELIMITED\n",
    "            FIELDS TERMINATED BY ','\n",
    "            STORED AS TEXTFILE\n",
    "        \"\"\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acest table poate fi văzut în HUE la secțiunea de tabele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../ss/airflow-section-03/section-03-ss-10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
