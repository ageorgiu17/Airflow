{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSubmitOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum că avem și tabelul putem să introducem date în acesta. Pentru a face asta, întâi trebuie să procesăm datele respective. Pentru a procesa datele o să folosim Spark (tot de la Apache, precum Airflow). Airflow este un sistem de orchestrare, nu unul de procesare a datelor, prin urmare nu trebuie să procesăm datele pe serverul de Ariflow. Ca să facem această procesare o să ne folosim de un cluster de Spark (creat și rulat când am porit toate aplicațiile). Ca să realizăm această procesare o să ne folosim de operatorul SparkSubmitOperator. Acesta are nevoie de următoarele:\n",
    "\n",
    "- application = fișierul care să fie rulat în Spark\n",
    "\n",
    "- conn_id = conexiunea către Spark\n",
    "\n",
    "- verbose (opțional) = ca să nu apară multe loguri din Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicația care o să ruleze pe spark o să fie salvată într-un fișier separat de python. Codul este următorul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Forex processing\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the file forex_rates.json from the HDFS\n",
    "df = spark.read.json('hdfs://namenode:9000/forex/forex_rates.json')\n",
    "\n",
    "# Drop the duplicated rows based on the base and last_update columns\n",
    "forex_rates = df.select('base', 'last_update', 'rates.eur', 'rates.usd', 'rates.cad', 'rates.gbp', 'rates.jpy', 'rates.nzd') \\\n",
    "    .dropDuplicates(['base', 'last_update']) \\\n",
    "    .fillna(0, subset=['EUR', 'USD', 'JPY', 'CAD', 'GBP', 'NZD'])\n",
    "\n",
    "# Export the dataframe into the Hive table forex_rates\n",
    "forex_rates.write.mode(\"append\").insertInto(\"forex_rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cadrul fișierului de mai sus este codul care o să ruleze utilizând Spark. Codul respectiv o să citească fișierul de date forex_rates.json (pe care l-am adăugat în HDFS) și o să facă drop la elementele care sunt duplicate. După ce se realizează această procesare o să fie introduse datele în tabelul forex_rates (tabel realizat cu operatorul HiveOperator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum că avem codul de procesare din Spark, trebuie să ne creem și conexiunea către clusterul de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../ss/airflow-section-03/section-03-ss-11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexinea din Spark este următoarea. După cum spuneam, clusterul de Spark a fost creat cu fișierul acela de docker-compose, iar datele respective au fost trecute în acel fișier (datele pentru host și port)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum că avem datele putem să ne creem acest operator din Spark pentru procesare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "\n",
    "process_forex_rates_spark = SparkSubmitOperator(\n",
    "    task_id=\"process_forex_rates_spark\",\n",
    "    conn_id=\"spark_conn\",\n",
    "    application=\"/opt/airflow/dags/scripts/forex_processing.py\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca să fie complet codul, trebuie doar să adaugăm acest cod în cadrul fișierului cu DAG-ul. Ca să verificăm dacă acest cod a fost rulat, putem să verificăm dacă există valori adăugate în cadrul tabelului pe care l-am creat anterior. Pentru a face asta o să ne folosim din nou de HUE, de partea de HQL unde o să rulăm comanda \"SELECT * FROM forex_rates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În HUE trebuie să vedem următorul rezultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../ss/airflow-section-03/section-03-ss-12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După cum se poate observa, există date în cadrul acestui tabel, ceea ce înseamnă că procesare prin Spark a funcționat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
