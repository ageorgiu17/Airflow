{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O să luăm un exemplu de ce este util acest tool de Airflow și la ce ne poate ajuta. Să ne imaginăm faptul că avem un pipeline (data pipeline), pipeline care trebuie să ruleze zilnic la o anumită oră. Dintre primele motive pentru care am folosi Airflow este să ne asigurăm că pipeline-urile rulează la o anumită oră specifică."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../ss/airflow-section-02/section-02-ss-01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Să presupunem că avem pipeline-ul de mai sus, pipeline care are 3 pași de executat.\n",
    "\n",
    "1. Download Data - acest task interacționează cu un anumit API extern pentru a descărca anumite date\n",
    "\n",
    "2. Process Data - în acest task se realizează partea de curățare și preprocesare de date. Aici se poate folosi Spark ca și sistem de procesare a datelor\n",
    "\n",
    "3. Store Data - în ultimul task se introduc datele într-o bază de date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fiecare dintre aceste task-uri este posibil să apară o anumită eroare, poate API-ul nu este disponibil în momentul în care se rulează pipeline-ul, poate framework-ul de Spark nu are memoria disponibilă sau s-a întâmplat ceva cu baza de date. Cum anume se pot managereia acești pași fără un tool de orchestrare precum Airflow? Airflow ne ajută (în special într-un pipeline mare cu mii de task-uri) să vedem exact unde anume a dat fail acest pipeline și din ce cauză."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
